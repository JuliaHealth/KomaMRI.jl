<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>GPU Parallelization ¬∑ KomaMRI.jl</title><meta name="title" content="GPU Parallelization ¬∑ KomaMRI.jl"/><meta property="og:title" content="GPU Parallelization ¬∑ KomaMRI.jl"/><meta property="twitter:title" content="GPU Parallelization ¬∑ KomaMRI.jl"/><meta name="description" content="Documentation for KomaMRI.jl."/><meta property="og:description" content="Documentation for KomaMRI.jl."/><meta property="twitter:description" content="Documentation for KomaMRI.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/hide-documenter-example-output.css" rel="stylesheet" type="text/css"/><link href="../../assets/center-images.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="KomaMRI.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="KomaMRI.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">üè† Home</a></li><li><a class="tocitem" href="../../how-to/1-getting-started/">üèÉ Getting Started</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">üèãÔ∏è Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../tutorial/01-FID/">Free Induction Decay</a></li><li><a class="tocitem" href="../../tutorial/02-SmallTipApproximation/">Small Tip Angle Approximation</a></li><li><a class="tocitem" href="../../tutorial/03-ChemicalShiftEPI/">Chemical Shift in an EPI sequence</a></li><li><a class="tocitem" href="../../tutorial/04-3DSliceSelective/">Slice-Selective Acquisition of 3D Phantom</a></li><li><a class="tocitem" href="../../tutorial/05-SimpleMotion/">Patient&#39;s Motion During Acquisition</a></li><li><a class="tocitem" href="../../tutorial/06-DiffusionMotion/">Diffusion-induced Signal Attenuation</a></li><li><a class="tocitem" href="../../tutorial/07-RRVariability/">Cardiac Cine MRI with Arrhythmias</a></li><li><a class="tocitem" href="../../tutorial/07-label/">Using Labels to reconstruct multi-slice / multi-contrast sequences</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">üßë‚Äçüî¨ Reproducible Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../tutorial-pluto/01-gradient-echo-spin-echo/">Understanding basic MRI sequences</a></li><li><a class="tocitem" href="../../tutorial-pluto/02-low-field-cmra-optimization/">Low-Field CMRA Optimization</a></li><li><a class="tocitem" href="../../tutorial-pluto/03-low-field-boost-optimization/">Low-Field BOOST Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">üë®‚Äçüç≥ How to</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../how-to/2-1-use-koma-ui/">Use Koma&#39;s User Interface</a></li><li><a class="tocitem" href="../../how-to/2-2-use-koma-notebooks/">Use Koma in Notebooks</a></li><li><a class="tocitem" href="../../how-to/2-3-use-koma-scripts/">Use Koma in Julia Scripts</a></li><li><a class="tocitem" href="../../how-to/3-create-your-own-phantom/">Create Your Own Phantom</a></li><li><a class="tocitem" href="../../how-to/3-create-your-own-sequence/">Create Your Own Sequence</a></li><li><a class="tocitem" href="../../how-to/4-run-distributed-simulations/">Run Distributed Simulations</a></li><li><a class="tocitem" href="../../how-to/5-contribute-to-koma/">Contribute to Koma</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox" checked/><label class="tocitem" for="menuitem-6"><span class="docs-label">ü§î Explanations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../1-phantom/">Phantom</a></li><li><a class="tocitem" href="../2-motion/">Motion</a></li><li><a class="tocitem" href="../3-phantom-format/">Phantom File Format</a></li><li><a class="tocitem" href="../4-sequence/">Sequence</a></li><li><a class="tocitem" href="../5-seq-events/">Sequence Events</a></li><li><a class="tocitem" href="../6-simulation/">Simulation</a></li><li class="is-active"><a class="tocitem" href>GPU Parallelization</a><ul class="internal"><li><a class="tocitem" href="#Choosing-a-GPU-Backend"><span>Choosing a GPU Backend</span></a></li><li><a class="tocitem" href="#How-Objects-are-moved-to-the-GPU"><span>How Objects are moved to the GPU</span></a></li><li><a class="tocitem" href="#Inside-the-Simulation"><span>Inside the Simulation</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">üë®‚Äçüíª Reference Guides</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../reference/1-api/">API Overview</a></li><li><a class="tocitem" href="../../reference/2-koma-base/">KomaMRIBase</a></li><li><a class="tocitem" href="../../reference/3-koma-core/">KomaMRICore</a></li><li><a class="tocitem" href="../../reference/4-koma-files/">KomaMRIFiles</a></li><li><a class="tocitem" href="../../reference/5-koma-plots/">KomaMRIPlots</a></li><li><a class="tocitem" href="../../reference/6-koma-mri/">KomaMRI</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">ü§î Explanations</a></li><li class="is-active"><a href>GPU Parallelization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>GPU Parallelization</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaHealth/KomaMRI.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaHealth/KomaMRI.jl/blob/master/docs/src/explanation/7-gpu-explanation.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="GPU-Parallelization"><a class="docs-heading-anchor" href="#GPU-Parallelization">GPU Parallelization</a><a id="GPU-Parallelization-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Parallelization" title="Permalink"></a></h1><p>KomaMRI uses a vendor agnostic approach to GPU parallelization in order to support multiple GPU backends. Currently, the following backends are supported:</p><ul><li>CUDA.jl (Nvidia)</li><li>Metal.jl (Apple)</li><li>AMDGPU.jl (AMD)</li><li>oneAPI.jl (Intel)</li></ul><h2 id="Choosing-a-GPU-Backend"><a class="docs-heading-anchor" href="#Choosing-a-GPU-Backend">Choosing a GPU Backend</a><a id="Choosing-a-GPU-Backend-1"></a><a class="docs-heading-anchor-permalink" href="#Choosing-a-GPU-Backend" title="Permalink"></a></h2><p>To determine which backend to use, KomaMRI uses <a href="https://pkgdocs.julialang.org/v1/creating-packages/#Conditional-loading-of-code-in-packages-(Extensions)">package extensions</a> (introduced in Julia 1.9) to avoid having the packages for each GPU backend as explicit dependencies. This means that the user is responsible for loading the backend package (e.g. <code>using CUDA</code>) at the beginning of their code, or prior to calling KomaUI(), otherwise, Koma will default back to the CPU:</p><pre><code class="language-julia hljs">using KomaMRI
using CUDA # loading CUDA will load KomaMRICoreCUDAExt, selecting the backend</code></pre><p>Once this is done, no further action is needed! The simulation objects will automatically be moved to the GPU and back once the simulation is finished. When the simulation is run a message will be shown with either the GPU device being used or the number of CPU threads if running on the CPU.</p><p>Of course, it is still possible to move objects to the GPU manually, and control precision using the f32 and f64 functions:</p><pre><code class="language-julia hljs">x = rand(100)
x |&gt; f32 |&gt; gpu # Float32 CuArray</code></pre><p>To change the precision level used for the entire simulation, the <code>sim_params[&quot;precision&quot;]</code> parameter can be set to either <code>f32</code> or <code>f64</code> (Note that for most GPUs, Float32 operations are considerably faster compared with Float64). In addition, the <code>sim_params[&quot;gpu&quot;]</code> option can be set to true or false to enable / disable the gpu functionality (if set to true, the backend package will still need to be loaded beforehand):</p><p>Two other simulation parameters, <code>gpu_groupsize_precession</code> and <code>gpu_groupsize_excitation</code> are exposed to allow adjusting the number of threads in each threadgroup within the <code>run_spin_precession!</code> and <code>run_spin_excitation!</code> gpu kernels. By default, they are both 256, however, on some devices other values may result in faster performance. The gpu groupsize must be a multiple of 32 between 32 and no higher than 1024, otherwise the simulation will throw an error.</p><pre><code class="language-julia hljs">using KomaMRI
using CUDA
sys = Scanner
obj = brain_phantom2D()
seq = PulseDesigner.EPI_example()

#Simulate on the GPU using 32-bit floating point values
sim_params = Dict{String,Any}(
  &quot;Nblocks&quot; =&gt; 20,
  &quot;gpu&quot; =&gt; true,
  &quot;precision&quot; =&gt; &quot;f32&quot;
  &quot;sim_method&quot; =&gt; Bloch(),
)
simulate(obj, seq, sys; sim_params)</code></pre><h2 id="How-Objects-are-moved-to-the-GPU"><a class="docs-heading-anchor" href="#How-Objects-are-moved-to-the-GPU">How Objects are moved to the GPU</a><a id="How-Objects-are-moved-to-the-GPU-1"></a><a class="docs-heading-anchor-permalink" href="#How-Objects-are-moved-to-the-GPU" title="Permalink"></a></h2><p>Koma&#39;s <code>gpu</code> function implementation calls a separate <code>gpu</code> function with a backend parameter of type <code>&lt;:KernelAbstractions.GPU</code> for the backend it is using. This function then calls the <code>fmap</code> function from package <code>Functors.jl</code> to recursively call <code>adapt</code> from package <code>Adapt.jl</code> on each field of the object being transferred. This is similar to how many other Julia packages, such as <code>Flux.jl</code>, transfer data to the GPU. However, an important difference is that KomaMRI adapts directly to the <code>KernelAbstractions.Backend</code> type in order to use the <code>adapt_storage</code> functions defined in each backend package, rather than defining custom adapters, resulting in an implementation with fewer lines of code.</p><h2 id="Inside-the-Simulation"><a class="docs-heading-anchor" href="#Inside-the-Simulation">Inside the Simulation</a><a id="Inside-the-Simulation-1"></a><a class="docs-heading-anchor-permalink" href="#Inside-the-Simulation" title="Permalink"></a></h2><p>KomaMRI has three different simulation methods, all of which can run on the GPU: </p><ul><li><code>BlochSimple</code>: <a href="https://github.com/JuliaHealth/KomaMRI.jl/blob/master/KomaMRICore/src/simulation/SimMethods/BlochSimple/BlochSimple.jl">BlochSimple.jl</a></li><li><code>BlochDict</code>: <a href="https://github.com/JuliaHealth/KomaMRI.jl/blob/master/KomaMRICore/src/simulation/SimMethods/BlochDict/BlochDict.jl">BlochDict.jl</a></li><li><code>Bloch</code>: <a href="https://github.com/JuliaHealth/KomaMRI.jl/blob/master/KomaMRICore/src/simulation/SimMethods/Bloch/BlochCPU.jl">BlochCPU.jl</a> / <a href="https://github.com/JuliaHealth/KomaMRI.jl/blob/master/KomaMRICore/src/simulation/SimMethods/Bloch/BlochGPU.jl">BlochGPU.jl</a></li></ul><p><code>BlochSimple</code> uses array-based methods which are simpler to understand compared with the more optimized <code>Bloch</code> implementation.</p><p><code>BlochDict</code> can be understood as an extension to <code>BlochSimple</code> that outputs a more detailed signal.</p><p><code>Bloch</code> is equivalent to <code>BlochSimple</code> in the operations it performs, but has separate implementations optimized for both the CPU and GPU. The CPU implementation uses array broadcasting for computation and preallocates all simulation arrays to conserve memory. The simulation arrays are 1-dimensional with length equal to the number of spins in the phantom and are updated at each time step. The GPU implementation also uses preallocation and a similar loop-based computation strategy, but does so using kernels for spin precession and excitation implemented using the <code>KernelAbstractions.jl</code> package. A key advantage of using kernel-based methods is that intermediate values compuated based on phantom and sequence properties can be stored in registers without having to write back to GPU global memory, which has much higher memory latency compared with the CPU. Other optimizations within the kernels include:</p><ul><li><p>Reducing the output signal value at each time step within the kernel so that the first thread for each thread group writes the sum of the signal values for each thread in the threadgroup to GPU global memory. This reduces the number of GPU global memory reads + writes needed for the output signal from <code>Number of Spins x Number of Time Points</code> to <code>Number of Spins x Number of Time Points / Number of Threads in Threadgroup</code>, improving scalability for large phantom objects.</p></li><li><p>Using julia&#39;s <code>Val</code> type to specialize at compile-time on properties unique to the simulation inputs. For example, whether the phantom exibits spin motion is passed as either <code>Val(true)</code> or <code>Val(false)</code> to the precession kernel so that different kernels will be compiled for phantoms with or without motion. For the kernel compiled for phantoms without motion, there will be no runtime check of the motion type of the phantom, and everything inside the <code>if MOTION</code> statements in the kernel will be compiled out, saving register space and enabling further compiler optimizations. This strategy enables adding support in the future for less common use cases without negatively impacting performance for simulations not using these features.</p></li><li><p>Since GPU registers are limited and can hurt GPU occupancy if a kernel uses a high number, their use is minimized by working with real and imaginary components directly rather than abstracting complex number math, using unsigned int32 literal values instead of Julia&#39;s default <code>Int64</code>, and inlining all functions called from within the kernels.</p></li></ul><p>The performance differences between Bloch and BlochSimple can be seen on the KomaMRI <a href="https://juliahealth.org/KomaMRI.jl/benchmarks/">benchmarks page</a>. The first data point is from when <code>Bloch</code> was what is now <code>BlochSimple</code>, before a more optimized implementation was created. The following pull requests are primarily responsible for the performance differences between <code>Bloch</code> and <code>BlochSimple</code>:</p><ul><li><a href="https://github.com/JuliaHealth/KomaMRI.jl/pull/443">(443) Optimize run<em>spin</em>precession! and run<em>spin</em>excitation! for CPU</a></li><li><a href="https://github.com/JuliaHealth/KomaMRI.jl/pull/459">(459) Optimize run<em>spin</em>precession! for GPU</a></li><li><a href="https://github.com/JuliaHealth/KomaMRI.jl/pull/462">(462) Optimize run<em>spin</em>excitation! for GPU</a></li><li><a href="https://github.com/JuliaHealth/KomaMRI.jl/pull/537">(537) Faster Bloch GPU</a></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../6-simulation/">¬´ Simulation</a><a class="docs-footer-nextpage" href="../../reference/1-api/">API Overview ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Saturday 14 February 2026 23:52">Saturday 14 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
